{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%load /home/lianos91/workspace/SemesterProject/imageReader.py\n",
      "#%load /home/lianos91/workspace/SemesterProject/sequenceReader.py\n",
      "%load /home/lianos91/workspace/SemesterProject/myCNN.py\n",
      "%load /home/lianos91/workspace/SemesterProject/tensorNet.py\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "class TensorNet:\n",
      "    \n",
      "    def __init__(self,nclasses,istrainable=True,name=''):\n",
      "        self._nclasses = nclasses\n",
      "        #with tf.variable_scope(name):\n",
      "        self.x = tf.placeholder(tf.float32, [None,4096])\n",
      "        self.y = tf.placeholder(tf.int32, [None,2])\n",
      "        self.keep_prob = tf.placeholder(tf.float32)\n",
      "        self._istrainable = istrainable\n",
      "        self._scopename = name\n",
      "    def predict(self):\n",
      "        True\n",
      "    \n",
      "    def train(self):\n",
      "        True\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "\n",
      "class MyCNN(TensorNet):\n",
      "    \n",
      "    def __init__(self,nclasses,istrainable=True,name = ''):\n",
      "        super().__init__( nclasses,istrainable,name)\n",
      "        self.initializevars()\n",
      "        #with tf.variable_scope(self._scopename):\n",
      "        self.y = tf.placeholder(tf.float32, [None,2])\n",
      "\n",
      "        \"\"\"predictions for individual frames\"\"\"\n",
      "        self.logits,_,_,_,_ = self.inference(self.x, self.weights, self.biases, self.keep_prob)\n",
      "        logits = self.logits\n",
      "        self.predictions = tf.argmax(logits,1)\n",
      "\n",
      "        #\"\"\"prediction for a single sequence\"\"\"\n",
      "        #self.sequence_prediction = tf.greater_equal(tf.reduce_mean(self.predictions),\n",
      "        #                                           tf.constant(tf.shape(self.predictions)[0]))\n",
      "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.logits, self.y))\n",
      "\n",
      "    def calc_cost(self):\n",
      "        \"\"\"evaluate accuracy on indiv. frames\"\"\"\n",
      "        \n",
      "        correct_pred = tf.equal(self.predictions, tf.argmax(self.y,1))\n",
      "        batchaccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))    \n",
      "        return self.cost, batchaccuracy, self.predictions\n",
      "    \n",
      "    \"\"\"prediction for a single sequence\"\"\"    \n",
      "    def sequence_predict(self,sess,seq_xs,dropout,seqlen,seq_ys = []):\n",
      "\n",
      "        \n",
      "        \"\"\"must transform seq_ys to one-hot, IF USE TF FOR CORRECTION LABELLING\"\"\"\n",
      "        if len(seq_ys) > 0:\n",
      "            # since we have 1 sequence\n",
      "            seq_ys = seq_ys[0,0:seqlen[0]]\n",
      "            y = np.zeros([seqlen[0],2])\n",
      "            y[np.arange(0,seqlen[0]),np.array(seq_ys,dtype=np.int32)] = 1\n",
      "\n",
      "        \"\"\"cut spare entries of xs (added by the reader)\"\"\"\n",
      "        seq_xs = seq_xs[0:seqlen[0],:]\n",
      "        \n",
      "        cost = -1\n",
      "        if len(seq_ys) > 0:\n",
      "            llogits, predictions,cost = sess.run( [self.logits, self.predictions,self.cost], feed_dict={self.x: seq_xs, self.y: y, \n",
      "                            self.keep_prob: dropout})      \n",
      "        else:\n",
      "            llogits,predictions = sess.run( [self.logits, self.predictions], feed_dict={self.x: seq_xs, \n",
      "                            self.keep_prob: dropout})      \n",
      "       \n",
      "        #print('--')\n",
      "        #print(seqlen[0])\n",
      "        #print(np.sum(predictions))\n",
      "        #print(seq_ys[0])\n",
      "        seq_prediction = np.sum(predictions) >= seqlen[0]/2.\n",
      "        #print(l)\n",
      "        #print(predictions)\n",
      "        #print(seq_prediction)\n",
      "        #print(seq_ys) \n",
      "        \n",
      "        #if seq_ys is provided, then output also correct predictions\n",
      "        corr_preds = []\n",
      "        if len(seq_ys) > 0:\n",
      "            corr_preds = (seq_ys[0] == seq_prediction)\n",
      "\n",
      "        return np.sum(llogits,0),seq_prediction, [corr_preds], cost \n",
      "    \n",
      "    \n",
      "    def seq_acc(self):\n",
      "        pred,_,_,_ = self.inference(self.x, self.weights, self.biases, self.keep_prob)\n",
      "        pred = tf.argmax(pred,1)\n",
      "        \n",
      "        sequence_prediction = tf.greater_equal(tf.reduce_mean(pred),tf.constant(tf.size(pred)[0]))\n",
      "        correct_preds = tf.equal(pred, tf.argmax(self.y,1))\n",
      "        acc = tf.reduce_mean(tf.cast(correct_preds, tf.float32))    \n",
      "        res = tf.greater_equal(acc,tf.div(tf.cast(tf.size(acc), tf.float32),tf.constant(2.0)))\n",
      "        return res \n",
      "        \n",
      "    def conv2d(self,name, l_input, w, b,s=1):\n",
      "            return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(l_input, w, strides=[1, s, s, 1], padding='SAME'),b), name=name)\n",
      "         \n",
      "    def max_pool(self,name, l_input, k):\n",
      "            return tf.nn.max_pool(l_input, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME', name=name)\n",
      "         \n",
      "    def norm(self,name, l_input, lsize=4):\n",
      "            return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)\n",
      "        \n",
      "    def inference(self,_X, _weights, _biases, _dropout):\n",
      "        # Reshape input picture\n",
      "        _X = tf.reshape(_X, shape=[-1, 64, 64, 1])\n",
      "    \n",
      "        # Convolution Layer\n",
      "        conv1 = self.conv2d('conv1', _X, _weights['wc1'], _biases['bc1'])\n",
      "        # Max Pooling (down-sampling)\n",
      "        pool1 = self.max_pool('pool1', conv1, k=2)\n",
      "        # Apply Normalization\n",
      "        norm1 = self.norm('norm1', pool1, lsize=4)\n",
      "    \n",
      "        # Convolution Layer\n",
      "        conv2 = self.conv2d('conv2', norm1, _weights['wc2'], _biases['bc2'])\n",
      "        # Max Pooling (down-sampling)\n",
      "        pool2 = self.max_pool('pool2', conv2, k=2)\n",
      "        # Apply Normalization\n",
      "        norm2 = self.norm('norm2', pool2, lsize=4)\n",
      "    \n",
      "        # Convolution Layer\n",
      "        conv3 = self.conv2d('conv3', norm2, _weights['wc3'], _biases['bc3'])\n",
      "       \n",
      "        conv4 = self.conv2d('conv4', conv3, _weights['wc4'], _biases['bc4'])\n",
      "\n",
      "        conv5 = self.conv2d('conv3', conv4, _weights['wc5'], _biases['bc5']) \n",
      "        pool5 = self.max_pool('pool5', conv5, k=2)\n",
      "\n",
      "        # Fully connected layer\n",
      "        dense1 = tf.reshape(pool5, [-1, _weights['wd1'].get_shape().as_list()[0]]) # Reshape pool5 output to fit dense layer input\n",
      "        dense1 = tf.nn.relu(tf.matmul(dense1, _weights['wd1']) + _biases['bd1'], name='fc1') # Relu activation\n",
      "        dense1 = tf.nn.dropout(dense1,_dropout)\n",
      "        \n",
      "        dense2 = tf.nn.relu(tf.matmul(dense1, _weights['wd2']) + _biases['bd2'], name='fc2') # Relu activation\n",
      "        dense2 = tf.nn.dropout(dense2,_dropout)\n",
      "        \n",
      "        dense3 = tf.nn.relu(tf.matmul(dense2, _weights['wd3']) + _biases['bd3'], name='fc3') # Relu activation\n",
      "        dense3 = tf.nn.dropout(dense3,_dropout)\n",
      "        \n",
      "        # Output, class logits\n",
      "        out = tf.matmul(dense3, _weights['out']) + _biases['out']\n",
      "        \n",
      "        self.out = out\n",
      "        self.dense1 = dense1\n",
      "        self.conv5 = conv5\n",
      "        \n",
      "        return out,pool5,dense1,dense2,dense3\n",
      "    \n",
      "    # Store layers weight & bias\n",
      "    def initializevars(self):\n",
      "            self.weights = {\n",
      "                'wc1': tf.Variable(tf.random_normal([3, 3, 1, 128],stddev=0.01),trainable = self._istrainable),\n",
      "                'wc2': tf.Variable(tf.random_normal([3, 3, 128, 128],stddev=0.01),trainable = self._istrainable),\n",
      "                'wc3': tf.Variable(tf.random_normal([3, 3, 128, 128],stddev=0.01),trainable = self._istrainable),\n",
      "                'wc4': tf.Variable(tf.random_normal([3, 3, 128, 128],stddev=0.01),trainable = self._istrainable),\n",
      "                'wc5': tf.Variable(tf.random_normal([3, 3, 128, 128],stddev=0.01),trainable = self._istrainable),\n",
      "                'wd1': tf.Variable(tf.random_normal([np.int(np.ceil(64*64/(8*8))*128), 400],stddev=0.01),trainable = self._istrainable),\n",
      "                'wd2': tf.Variable(tf.random_normal([400, 400],stddev=0.01),trainable = self._istrainable),\n",
      "                'wd3': tf.Variable(tf.random_normal([400, 200],stddev=0.01),trainable = self._istrainable),\n",
      "                'out': tf.Variable(tf.random_normal([200, self._nclasses],stddev=0.01),trainable = self._istrainable)\n",
      "            }\n",
      "            self.biases = {\n",
      "                'bc1': tf.Variable(tf.zeros([128]),trainable = self._istrainable),\n",
      "                'bc2': tf.Variable(tf.zeros([128]),trainable = self._istrainable),\n",
      "                'bc3': tf.Variable(tf.zeros([128]),trainable = self._istrainable),\n",
      "                'bc4': tf.Variable(tf.zeros([128]),trainable = self._istrainable),\n",
      "                'bc5': tf.Variable(tf.zeros([128]),trainable = self._istrainable),\n",
      "                'bd1': tf.Variable(tf.zeros([400]),trainable = self._istrainable),\n",
      "                'bd2': tf.Variable(tf.zeros([400]),trainable = self._istrainable),\n",
      "                'bd3': tf.Variable(tf.zeros([200]),trainable = self._istrainable),\n",
      "                'out': tf.Variable(tf.zeros([self._nclasses]),trainable = self._istrainable)\n",
      "            }     \n",
      "          \n",
      "def CNN_sequence_predict(cnn,sess,seq_xs,dropout,seqlen,seq_ys = []):\n",
      " \n",
      "        \"\"\"must transform seq_ys to one-hot, IF USE TF FOR CORRECTION LABELLING\"\"\"\n",
      "        if len(seq_ys) > 0:\n",
      "            y = np.zeros([seqlen[0],2])\n",
      "            y[np.arange(0,seqlen[0]),np.array(seq_ys,dtype=np.int32)] = 1\n",
      " \n",
      "        cost = -1\n",
      "        if len(seq_ys) > 0:\n",
      "            llogits, predictions,cost = sess.run( [cnn.logits, cnn.predictions,cnn.cost], feed_dict={cnn.x: seq_xs, cnn.y: y, \n",
      "                            cnn.keep_prob: dropout})      \n",
      "        else:\n",
      "            llogits,predictions = sess.run( [cnn.logits, cnn.predictions], feed_dict={cnn.x: seq_xs, \n",
      "                            cnn.keep_prob: dropout})    \n",
      "        \n",
      "        seq_prediction = np.sum(predictions) >= seqlen[0]/2.\n",
      "        \n",
      "        #if seq_ys is provided, then output also correct predictions\n",
      "        corr_preds = []\n",
      "        if len(seq_ys) > 0:\n",
      "            corr_preds = (seq_ys[0] == seq_prediction)\n",
      "\n",
      "        return np.sum(llogits,0),seq_prediction, [corr_preds], cost \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import os\n",
      "from scipy import misc\n",
      "import numpy as np\n",
      "from random import shuffle\n",
      "\n",
      "class ImageReader:\n",
      "    \n",
      "    #_dirpath = \"/home/lianos91/Desktop/training_patches/train_patches_128_64/\"\n",
      "    #fname = \"/home/lianos91/Desktop/training_patches/train_patches_128_64/train.txt\"\n",
      "\n",
      "    \n",
      "    def __init__(self,dirpath,datapath,istest,imdim=64,isComposite = False):\n",
      "        self._index = 0\n",
      "        self._dirpath = dirpath + datapath\n",
      "        \n",
      "        self.mean_img = misc.imread(dirpath+'train_mean.png').astype(np.float32)\n",
      "\n",
      "        self._iscomposite = isComposite\n",
      "\n",
      "        self._epochs = 0\n",
      "        self._istest = istest   \n",
      "        np.random.seed()     \n",
      "        self._epochIncr = 0\n",
      "        if istest:\n",
      "            fname = \"/home/lianos91/Desktop/training_patches/train_patches_128_64/test.txt\"\n",
      "        #with open(fname) as f:\n",
      "        #    content = f.readlines()\n",
      "            \n",
      "        #b = [i.split(\" \")[0] for i in content]\n",
      "        #self._fileslist = [i.split(\"/\")[2] for i in b]\n",
      "        self._fileslist = next(os.walk(self._dirpath))[2]\n",
      "        self._dirsize = len(self._fileslist)\n",
      "        \n",
      "        self._imdim = imdim\n",
      "        self._imsize = imdim*imdim\n",
      "        \n",
      "        \n",
      "        \n",
      "    def read_batch(self,batchsize):\n",
      "        \n",
      "        imsequence = np.zeros([batchsize, self._imsize])\n",
      "        labelsequence = np.zeros([batchsize,2])\n",
      "        \n",
      "        for i in range(batchsize):\n",
      "            jitt = -1\n",
      "            flip = np.round(np.random.random()-0.2)\n",
      "            \n",
      "            im,lbl = self.read_single_file(flip,jitt)\n",
      "            \n",
      "            imsequence[i,:] = im\n",
      "            labelsequence[i,lbl] = 1\n",
      "\n",
      "        if self._epochIncr == 1:\n",
      "            self._epochIncr = 0\n",
      "            shuffle(self._fileslist)\n",
      "            self._index = 0\n",
      "\n",
      "        return imsequence,labelsequence        \n",
      "\n",
      "\n",
      "    def invoke_thread(self):\n",
      "        True\n",
      "\n",
      "    \"\"\"Reads a single file from the list, incrementing the index\"\"\"\n",
      "    \"\"\"Arguments:\n",
      "        flip: if true, then flip the image left-to-right\n",
      "        \n",
      "        jitt: if -1,       then randomly displace the image by [0,4]\n",
      "              if in [0,4], then displace by that amount\n",
      "              if > 4,      no displacement\n",
      "    \"\"\"\n",
      "    def read_single_file(self,flip,jitt = 0,imdim = 64):\n",
      "        file = self._fileslist[self._index]\n",
      "        img = misc.imread(self._dirpath+file)\n",
      "        \n",
      "        self._index = (self._index + 1) % (self._dirsize)\n",
      "        if self._index == 0:\n",
      "            print(\"Epoch complete\")\n",
      "            self._epochs += 1\n",
      "            self._epochIncr = 1\n",
      "            \n",
      "        # get input image\n",
      "        img.astype(np.float32)\n",
      "        \n",
      "        img = img - self.mean_img \n",
      "    \n",
      "        #parse tokens\n",
      "        tokens = file.split(\"_\")\n",
      "        tokens[len(tokens)-1] = tokens[len(tokens)-1].split(\".\")[0]    \n",
      "        \n",
      "        imlabel = tokens[4]  \n",
      "       \n",
      "        if flip:\n",
      "            a = np.copy(img)\n",
      "            for i in range(64):\n",
      "                a[:,i] = img[:,-i-1]\n",
      "            img = a\n",
      "        \n",
      "        b = np.copy(imdim)\n",
      "        #jitter random\n",
      "        if (not self._istest) and (jitt < 5):\n",
      "            q = np.floor(np.random.rand()*4.9)\n",
      "            if jitt >= 0:\n",
      "                q = jitt\n",
      "            a = img[q: 64-(4-q), q: 64-(4-q)]\n",
      "            b=np.pad(a,(2,2),'edge')\n",
      "        \n",
      "        #img = b\n",
      "\n",
      "        #crop it\n",
      "        q = 64 - imdim\n",
      "    \n",
      "        if q > 0:\n",
      "            img = b[q/2: 64-q/2, q/2: 64-q/2]\n",
      "        \n",
      "        if self._iscomposite:\n",
      "            img = np.reshape(np.copy(img),[1,imdim*imdim,3])\n",
      "        else:\n",
      "            img = np.reshape(np.copy(img),imdim*imdim)\n",
      "\n",
      "    \n",
      "        return img,imlabel  \n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "class SequenceReader(ImageReader):\n",
      "\n",
      "    def __init__(self,dirpath,datapath,istest,maxsequence,imdim = 64,isComposite = False):\n",
      "        super().__init__( dirpath,datapath,istest,imdim,isComposite)\n",
      "        self._fileslist.sort()\n",
      "        self._maxseq = maxsequence\n",
      "        self._ignoresmall = 1\n",
      "        \n",
      "    def read_batch(self,batchsize):\n",
      "        \n",
      "        imsequence = np.zeros([batchsize*self._maxseq,self._imsize])\n",
      "        if self._iscomposite:\n",
      "            imsequence = np.zeros([batchsize*self._maxseq,self._imsize,3])\n",
      "        labelsequence = np.zeros([batchsize,self._maxseq])\n",
      "        sequencelength =[]\n",
      "        \"\"\"imsequence: outer_idx: maxseq, inner_idx: batchsize\"\"\"\n",
      "        \"\"\"labelsequence: batch_id X maxseq\"\"\"\n",
      "        i = 0\n",
      "        c = np.zeros([2,])\n",
      "        c[1] = np.ceil(batchsize/2.)\n",
      "        c[0] = np.floor(batchsize/2.)\n",
      "        while i < batchsize:\n",
      "            ids = [j for j in range(i,batchsize*self._maxseq,batchsize)]\n",
      "            seqlen,im,lbl = self.read_sequence()\n",
      "            \"\"\"ignore small sequences!!\"\"\"\n",
      "            if seqlen < self._ignoresmall:\n",
      "                continue\n",
      "            \n",
      "            \"\"\"get batches with equal positive and negative class samples only when training\"\"\"\n",
      "            if (not self._istest) and (batchsize > 1):\n",
      "                if c[lbl[0]] == 0:\n",
      "                    continue\n",
      "                else:\n",
      "                    c[lbl[0]] = c[lbl[0]] - 1\n",
      "            imsequence[ids,:] = im\n",
      "            labelsequence[i] = lbl\n",
      "            sequencelength.append(seqlen)\n",
      "            i = i + 1\n",
      "\n",
      "        return sequencelength,imsequence,labelsequence        \n",
      "        \n",
      "    def read_sequence(self):\n",
      "        while True:\n",
      "            file = self._fileslist[self._index]\n",
      "            tokens = file.split(\"_\")\n",
      "            tokens[len(tokens)-1] = tokens[len(tokens)-1].split(\".\")[0]    \n",
      "            seqLen  = int(tokens[3])\n",
      "            pivot_name = tokens[0]\n",
      "            pivot_seqid = tokens[1]\n",
      "            \n",
      "            if self._istest:\n",
      "                break\n",
      "            p = np.random.rand()\n",
      "            if p > 0.4:\n",
      "                break\n",
      "            #progress to the next sequence\n",
      "            for i in range (0,seqLen+1):            \n",
      "                file2 = self._fileslist[(self._index+i)%self._dirsize]\n",
      "                testtokens = file2.split(\"_\")\n",
      "                testtokens[len(tokens)-1] = testtokens[len(tokens)-1].split(\".\")[0]    \n",
      "                if pivot_name != testtokens[0] or pivot_seqid != testtokens[1]:\n",
      "                    break        \n",
      "            self._index = (self._index +i) % self._dirsize\n",
      "        \n",
      "        #read the pivot sequence\n",
      "        \n",
      "        imseq = np.zeros([seqLen,self._imsize])\n",
      "        if self._iscomposite:\n",
      "            imseq = np.zeros([seqLen,self._imsize,3])\n",
      "        labelseq = np.zeros([seqLen])\n",
      "        \n",
      "        #randomly flip and crop the whole sequence\n",
      "        flipall = 0\n",
      "        impadding = 0\n",
      "        if not self._istest:\n",
      "            impadding = np.floor(np.random.rand()*4.9) \n",
      "            p = np.random.rand()\n",
      "            if p < 0.35:\n",
      "                flipall = 1\n",
      "    \n",
      "        ind = []\n",
      "        \n",
      "        for i in range (0,seqLen+1):\n",
      "            file = self._fileslist[self._index]\n",
      "            tokens = file.split(\"_\")\n",
      "            tokens[len(tokens)-1] = tokens[len(tokens)-1].split(\".\")[0]    \n",
      "            if pivot_name == tokens[0] and pivot_seqid == tokens[1]:\n",
      "                im,lbl = self.read_single_file(flipall,impadding,self._imdim) # increments the index\n",
      "                imseq[i,:] = im\n",
      "                labelseq[i] = lbl\n",
      "                ind.append(int(tokens[2]))\n",
      "            else:\n",
      "                break\n",
      "        \n",
      "        self.trueseqlen = seqLen \n",
      "        seqLen = i\n",
      "        #indices ind are like 0,1,10,11,..,2,20,..\n",
      "        #    so sort them\n",
      "        sort_index = np.argsort(ind)     \n",
      "        labelseq = np.copy(labelseq[sort_index])\n",
      "        imseq = np.copy(imseq[sort_index,:])\n",
      "        \n",
      "        imsequence = np.zeros([self._maxseq,self._imsize])\n",
      "        if self._iscomposite:\n",
      "            imsequence = np.zeros([self._maxseq,self._imsize,3])\n",
      "\n",
      "        labelsequence = np.zeros([self._maxseq])\n",
      "        \n",
      "        if seqLen == self._maxseq:\n",
      "            imsequence = imseq\n",
      "            labelsequence = labelseq\n",
      "        if seqLen < self._maxseq:\n",
      "            #padd \n",
      "            imsequence[0:seqLen,:] = imseq[0:seqLen,:]\n",
      "            labelsequence[0:seqLen] = labelseq[0:seqLen]            \n",
      "            imsequence[seqLen::,:] = 0.#imseq[0,:]\n",
      "            labelsequence[seqLen::] = -1\n",
      "        if seqLen > self._maxseq:\n",
      "            #sample   \n",
      "            ids = self.seq_sampling(seqLen)\n",
      "            seqLen = len(ids)     \n",
      "            imsequence[0:seqLen,:] = np.copy(imseq[ids,:])\n",
      "            labelsequence[0:seqLen] = np.copy(labelseq[ids])\n",
      "            imsequence[seqLen::,:] = 0.#imseq[0,:]\n",
      "            labelsequence[seqLen::] = -1  \n",
      "\n",
      "        return seqLen,imsequence,labelsequence           \n",
      "    \n",
      "    \n",
      "    def seq_sampling(self,seqlen):\n",
      "        mseq = self._maxseq\n",
      "        sampling_rate = (seqlen/mseq)\n",
      "        c = [int(np.round(i)) for i in np.arange(0,seqlen,sampling_rate)]\n",
      "        return c\n",
      "    \n",
      "            \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import os\n",
      "from scipy import misc\n",
      "import matplotlib.gridspec as gridspec\n",
      "import matplotlib.pyplot as plt\n",
      "import csv\n",
      "import numpy as np\n",
      "\n",
      "LOAD_CNN_checkpoint_path = \"/home/lianos91/Desktop/training_patches/model9_2_125/\"\n",
      "\n",
      "USE_CNN = True\n",
      "\n",
      "dirpath = \"/home/lianos91/Desktop/training_patches/train_patches_128_64/\"\n",
      "datapath = \"val/\"\n",
      "testreader = SequenceReader(dirpath,datapath,istest=True,maxsequence=16,isComposite=False)\n",
      "\n",
      "\"\"\"Construct and load pretrained CNN model\"\"\"\n",
      "if USE_CNN:\n",
      "    import tensorflow as tf\n",
      "    CNNmodel = MyCNN(2,istrainable=True)\n",
      "    cost,batch_acc,preds = CNNmodel.calc_cost()\n",
      "    \n",
      "    optimizer = tf.train.AdamOptimizer(learning_rate=.001).minimize(cost)\n",
      "    \n",
      "    saverCNN = tf.train.Saver(tf.all_variables())\n",
      "    init1 = tf.initialize_all_variables()\n",
      "    \n",
      "    sess = tf.InteractiveSession()\n",
      "    \n",
      "    sess.run(init1)\n",
      "    \n",
      "    #print(sess.run(CNNmodel.weights['out']))\n",
      "    \n",
      "    ckpt = tf.train.get_checkpoint_state(LOAD_CNN_checkpoint_path)\n",
      "    if ckpt and ckpt.model_checkpoint_path:\n",
      "        print(ckpt.model_checkpoint_path)\n",
      "        print(\"[train_script]: LOADED CNN!\")\n",
      "        saverCNN.restore(sess, ckpt.model_checkpoint_path)\n",
      "    else:\n",
      "        print(\"[train_script]: Failed to LOAD CNN!\")\n",
      "        raise SystemExit  \n",
      "\n",
      "f1 = open(\"predictions_CNN.txt\", \"r\", newline='\\n')\n",
      "f2 = open(\"predictions_RNN.txt\", \"r\", newline='\\n')\n",
      "\n",
      "readerModel1 = reader = csv.reader(f1)\n",
      "readerModel2 = reader = csv.reader(f2)\n",
      "\n",
      "predsM1 = []\n",
      "for row in readerModel1:\n",
      "    #print(row[0])\n",
      "    predsM1.append(row[0]=='True')\n",
      "    \n",
      "predsM2 = []\n",
      "for row in readerModel2:\n",
      "    predsM2.append(row[0]=='True')\n",
      "    \n",
      "f1.close()\n",
      "f2.close()\n",
      "\n",
      "ind = 0    \n",
      "viz = 0\n",
      "testreader._ignoresmall = 4\n",
      "if not USE_CNN:\n",
      "    testreader.mean_img = np.zeros([64,64])\n",
      "\n",
      "plt.axis('off')    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "FileNotFoundError",
       "evalue": "[Errno 2] No such file or directory: 'predictions_CNN.txt'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-6-605217fcfdaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predictions_CNN.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predictions_RNN.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'predictions_CNN.txt'"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/lianos91/Desktop/training_patches/model9_2_125/myCNNadamlr0d0001.ckpt-12500\n",
        "[train_script]: LOADED CNN!\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "seqlen_test, seq_xs_test, seq_ys_test = testreader.read_batch(1)  \n",
      "label = seq_ys_test[0][0]\n",
      " \n",
      "seq_ys = seq_ys_test[0,0:seqlen_test[0]]\n",
      "seq_xs = seq_xs_test[0:seqlen_test[0],:]\n",
      "\n",
      "w = sess.run( [CNNmodel.weights['wc1']], \n",
      "                                 feed_dict={CNNmodel.x: seq_xs, \n",
      "                                 CNNmodel.keep_prob: 1.})\n",
      "\n",
      "w = w[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "D = np.shape(w)[3]\n",
      "a1 = int(np.floor(np.sqrt(D)))\n",
      "a2 = int(np.ceil(np.sqrt(D)))\n",
      "\n",
      "\n",
      "plt.axis('off')\n",
      "\n",
      "ff=plt.figure(1,figsize=(a1,a2))\n",
      "gs1 = gridspec.GridSpec(a1,a2)\n",
      "gs1.update(wspace=0.1, hspace=0) # set the spacing between axes. \n",
      "\n",
      "for i in range(a1): \n",
      "    for j in range(a2): \n",
      "        p1 = int(j+a2*i)\n",
      "        if p1 >= D:\n",
      "            break\n",
      "        a = ff.add_subplot(gs1[i,j])\n",
      "        #aa.axis('off')\n",
      "        a.set_xticklabels([])\n",
      "        a.set_yticklabels([])\n",
      "        #a.set_aspect('equal')\n",
      "        #print(p1)\n",
      "        plt.imshow(w[:,:,0,p1],cmap='Greys_r')\n",
      "\n",
      "#figManager = plt.get_current_fig_manager()\n",
      "#figManager.window.showMaximized()\n",
      "plt.show()\n",
      "#ff.savefig(\"compare_rnncnn_img\"+str(ind)+\".png\",bbox_inches='tight')\n",
      "#break\n",
      "\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}