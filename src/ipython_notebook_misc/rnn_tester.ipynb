{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%load /home/lianos91/workspace/SemesterProject/tensorNet.py\n",
      "#%load /home/lianos91/workspace/SemesterProject/myRNN.py\n",
      "#%load /home/lianos91/workspace/SemesterProject/myCNN.py\n",
      "\n",
      "#%load /home/lianos91/workspace/SemesterProject/myRNNreader.py\n",
      "#%load /home/lianos91/workspace/SemesterProject/myReader.py\n",
      "\n",
      "#%load /home/lianos91/workspace/SemesterProject/main_trainRnn.py\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "tf.reset_default_graph()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import os\n",
      "from scipy import misc\n",
      "\n",
      "class MyReader:\n",
      "    \n",
      "    _dirpath = \"/home/lianos91/Desktop/training_patches/train_patches_128_64/\"\n",
      "    mean_img = misc.imread(_dirpath+'train_mean.png').astype(np.float32)\n",
      "    imsize = 64\n",
      "    \n",
      "    def __init__(self,path,istest):\n",
      "        self._index = 0\n",
      "        self._dirpath = self._dirpath + path\n",
      "        self._fileslist = next(os.walk(self._dirpath))[2]\n",
      "        self._dirsize = len(self._fileslist)\n",
      "        self._epochs = 0\n",
      "        self._istest = istest   \n",
      "        np.random.seed(1)     \n",
      "        \n",
      "    def read_batch(self,batchsize, flipall = 0):\n",
      "        \n",
      "        imsequence = np.zeros([batchsize,4096])\n",
      "        labelsequence = np.zeros([batchsize])\n",
      "        \n",
      "        for i in range(batchsize):\n",
      "            im,lbl = self.read_single_file(flipall)\n",
      "            imsequence[i,:] = im\n",
      "            labelsequence[i] = lbl\n",
      "\n",
      "        return imsequence,labelsequence        \n",
      "\n",
      "    def invoke_thread(self):\n",
      "        True\n",
      "\n",
      "\n",
      "    def read_single_file(self,flip):\n",
      "        file = self._fileslist[self._index]\n",
      "        img = misc.imread(self._dirpath+file)\n",
      "        \n",
      "        self._index = (self._index + 1) % (self._dirsize)\n",
      "        if self._index == 0:\n",
      "            print(\"Epoch complete\")\n",
      "            self._epochs += 1\n",
      "            \n",
      "        # get input image\n",
      "        img.astype(np.float32)\n",
      "        img = img - self.mean_img \n",
      "    \n",
      "        #parse tokens\n",
      "        tokens = file.split(\"_\")\n",
      "        tokens[len(tokens)-1] = tokens[len(tokens)-1].split(\".\")[0]    \n",
      "        \n",
      "        imlabel = tokens[4]  \n",
      "       \n",
      "        if flip:\n",
      "            a = np.copy(img)\n",
      "            for i in range(64):\n",
      "                a[:,i] = img[:,-i-1]\n",
      "            img = a\n",
      "        \n",
      "        b = np.copy(img)\n",
      "        #jitter random\n",
      "        if not self._istest:\n",
      "            q = np.floor(np.random.rand()*4.9)\n",
      "            a = img[q: 64-(4-q), q: 64-(4-q)]\n",
      "            b=np.pad(a,(2,2),'edge')\n",
      "        \n",
      "       \n",
      "        \n",
      "        img = np.reshape(np.copy(b),4096)\n",
      "    \n",
      "        return img,imlabel  \n",
      "    \n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import numpy as np\n",
      "\n",
      "class MyRNNreader(MyReader):\n",
      "\n",
      "    def __init__(self,path,istest,maxsequence):\n",
      "        super().__init__( path,istest)\n",
      "        self._fileslist.sort()\n",
      "        self._maxseq = maxsequence\n",
      "        \n",
      "        \n",
      "    def read_batch(self,batchsize):\n",
      "        \n",
      "        imsequence = np.zeros([batchsize*self._maxseq,4096])\n",
      "        labelsequence = np.zeros([batchsize,self._maxseq])\n",
      "        sequencelength =[]\n",
      "        \"\"\"batch: outer idx, tstep: inner idx\"\"\"\n",
      "        for i in range(batchsize):\n",
      "            ids = [i for i in range(i,batchsize*self._maxseq,batchsize)]\n",
      "            seqlen,im,lbl = self.read_sequence()\n",
      "            imsequence[ids,:] = im\n",
      "            labelsequence[i] = lbl\n",
      "            sequencelength.append(seqlen)\n",
      "\n",
      "        \"\"\"Outer index: timestep, inner: batchid\"\"\"\n",
      "        return sequencelength,imsequence,labelsequence        \n",
      "        \n",
      "    def read_sequence(self):\n",
      "        while True:\n",
      "            file = self._fileslist[self._index]\n",
      "            tokens = file.split(\"_\")\n",
      "            tokens[len(tokens)-1] = tokens[len(tokens)-1].split(\".\")[0]    \n",
      "            seqLen  = int(tokens[3])\n",
      "            pivot_name = tokens[0]\n",
      "            pivot_seqid = tokens[1]\n",
      "            \n",
      "            if self._istest:\n",
      "                break\n",
      "            p = np.random.rand()\n",
      "            print(p)\n",
      "            if p > 0.4:\n",
      "                break\n",
      "            #progress to the next sequence\n",
      "            for i in range (0,seqLen+1):\n",
      "                file2 = self._fileslist[self._index+i]\n",
      "                testtokens = file2.split(\"_\")\n",
      "                testtokens[len(tokens)-1] = testtokens[len(tokens)-1].split(\".\")[0]    \n",
      "                if pivot_name != testtokens[0] or pivot_seqid != testtokens[1]:\n",
      "                    break        \n",
      "            self._index = (self._index +i) % self._dirsize\n",
      "        \n",
      "        \n",
      "        print(file)\n",
      "\n",
      "        imseq = np.zeros([seqLen,4096])\n",
      "        labelseq = np.zeros([seqLen])\n",
      "        \n",
      "        flipall = 0\n",
      "        if not self._istest:\n",
      "            p = np.random.rand()\n",
      "            if p < 0.35:\n",
      "                flipall = 1\n",
      "        \n",
      "        print(\"reading sequence..\")\n",
      "        ind = []\n",
      "        for i in range (0,seqLen+1):\n",
      "            file = self._fileslist[self._index]\n",
      "            tokens = file.split(\"_\")\n",
      "            print(file)\n",
      "\n",
      "            tokens[len(tokens)-1] = tokens[len(tokens)-1].split(\".\")[0]    \n",
      "            if pivot_name == tokens[0] and pivot_seqid == tokens[1]:\n",
      "                im,lbl = self.read_single_file(flipall) # increments the index\n",
      "                imseq[i,:] = im\n",
      "                labelseq[i] = lbl\n",
      "                ind.append(int(tokens[2]))\n",
      "            else:\n",
      "                break\n",
      "        \n",
      "        seqLen = i\n",
      "        print(\"seq. length: \"+str(seqLen))\n",
      "        sort_index = np.argsort(ind)     \n",
      "        labelseq = np.copy(labelseq[sort_index])\n",
      "        imseq = np.copy(imseq[sort_index,:])\n",
      "        \n",
      "        imsequence = np.zeros([self._maxseq,4096])\n",
      "        labelsequence = np.zeros([self._maxseq])\n",
      "        \n",
      "        if seqLen == self._maxseq:\n",
      "            imsequence = imseq\n",
      "            labelsequence = labelseq\n",
      "        if seqLen < self._maxseq:\n",
      "            #padd \n",
      "            imsequence[0:seqLen,:] = imseq[0:seqLen,:]\n",
      "            labelsequence[0:seqLen] = labelseq[0:seqLen]            \n",
      "            imsequence[seqLen::,:] = imseq[0,:]\n",
      "            labelsequence[seqLen::] = -1\n",
      "        if seqLen > self._maxseq:\n",
      "            #sample   \n",
      "            ids = self.seq_sampling(seqLen)\n",
      "            print(ids)\n",
      "            imsequence = np.copy(imseq[ids,:])\n",
      "            labelsequence = np.copy(labelseq[ids])\n",
      "            seqLen = self._maxseq       \n",
      "\n",
      "\n",
      "        return seqLen,imsequence,labelsequence           \n",
      "    \n",
      "    \n",
      "    def seq_sampling(self,seqlen):\n",
      "        p = np.array([i for i in range(seqlen)])\n",
      "        mseq = self._maxseq\n",
      "        sampling_rate = int(np.ceil(seqlen/mseq))\n",
      "        i = [j for j in range(0,seqlen,sampling_rate)]\n",
      "        jj=1\n",
      "        while not len(i) == mseq:\n",
      "            q = p[int(p.size/2) + jj]\n",
      "            if not q in i:\n",
      "                i.append(q)\n",
      "            if len(i)==mseq:\n",
      "                break\n",
      "            q = p[int(p.size/2) - jj]\n",
      "            if not q in i:\n",
      "                i.append(q) \n",
      "            jj+=1\n",
      "        i.sort()  \n",
      "        return i\n",
      "    \n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class TensorNet:\n",
      "    \n",
      "    def __init__(self,nclasses,istrainable=True):\n",
      "        self._nclasses = nclasses\n",
      "        self.x = tf.placeholder(tf.float32, [None,4096])\n",
      "        self.y = tf.placeholder(tf.int32, [None,2])\n",
      "        self.keep_prob = tf.placeholder(tf.float32)\n",
      "        self._istrainable = istrainable\n",
      "    def inference(self):\n",
      "        True\n",
      "    \n",
      "    def train(self):\n",
      "        True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "class MyCNN(TensorNet):\n",
      "    \n",
      "    def __init__(self,nclasses,istrainable=True):\n",
      "        super().__init__( nclasses,istrainable)\n",
      "        self.initializevars()\n",
      "        \n",
      "        self.y = tf.placeholder(tf.float32, [None,2])\n",
      "\n",
      "        \"\"\"predictions for individual frames\"\"\"\n",
      "        self.logits,_,_,_,_ = self.inference(self.x, self.weights, self.biases, self.keep_prob)\n",
      "        logits = self.logits\n",
      "        self.predictions = tf.argmax(logits,1)\n",
      "\n",
      "        #\"\"\"prediction for a single sequence\"\"\"\n",
      "        #self.sequence_prediction = tf.greater_equal(tf.reduce_mean(self.predictions),\n",
      "        #                                           tf.constant(tf.shape(self.predictions)[0]))\n",
      "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.logits, self.y))\n",
      "\n",
      "    def calc_cost(self):\n",
      "        \"\"\"evaluate accuracy on indiv. frames\"\"\"\n",
      "        \n",
      "        correct_pred = tf.equal(self.predictions, tf.argmax(self.y,1))\n",
      "        batchaccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))    \n",
      "        return self.cost, batchaccuracy, self.predictions\n",
      "    \n",
      "    \"\"\"prediction for a single sequence\"\"\"\n",
      "    def sequence_predict(self,sess,seq_xs,dropout,seqlen,seq_ys = []):\n",
      "\n",
      "        # since we have 1 sequence\n",
      "        seq_ys = seq_ys[0,0:seqlen[0]]\n",
      "        \n",
      "        \"\"\"must transform seq_ys to one-hot, IF USE TF FOR CORRECTION LABELLING\"\"\"\n",
      "        if len(seq_ys) > 0:\n",
      "            y = np.zeros([seqlen[0],2])\n",
      "            y[np.arange(0,seqlen[0]),np.array(seq_ys,dtype=np.int32)] = 1\n",
      "\n",
      "        \"\"\"cut spare entries of xs (added by the reader)\"\"\"\n",
      "        seq_xs = seq_xs[0:seqlen[0],:]\n",
      "        \n",
      "        cost = -1\n",
      "        if len(seq_ys) > 0:\n",
      "            l, predictions,cost = sess.run( [self.logits, self.predictions,self.cost], feed_dict={self.x: seq_xs, self.y: y, \n",
      "                            self.keep_prob: dropout})      \n",
      "        else:\n",
      "            predictions = sess.run( [self.predictions], feed_dict={self.x: seq_xs, \n",
      "                            self.keep_prob: dropout})      \n",
      "       \n",
      "        #print('--')\n",
      "        #print(seqlen[0])\n",
      "        #print(np.sum(predictions))\n",
      "        #print(seq_ys[0])\n",
      "        seq_prediction = np.sum(predictions) >= seqlen[0]/2.\n",
      "        #print(l)\n",
      "        #print(predictions)\n",
      "        #print(seq_prediction)\n",
      "        #print(seq_ys) \n",
      "        \n",
      "        #if seq_ys is provided, then output also correct predictions\n",
      "        corr_preds = []\n",
      "        if len(seq_ys) > 0:\n",
      "            corr_preds = (seq_ys[0] == seq_prediction)\n",
      "\n",
      "        return seq_prediction, [corr_preds], cost \n",
      "    \n",
      "    \n",
      "    def seq_acc(self):\n",
      "        pred,_,_,_ = self.inference(self.x, self.weights, self.biases, self.keep_prob)\n",
      "        pred = tf.argmax(pred,1)\n",
      "        \n",
      "        sequence_prediction = tf.greater_equal(tf.reduce_mean(pred),tf.constant(tf.size(pred)[0]))\n",
      "        correct_preds = tf.equal(pred, tf.argmax(self.y,1))\n",
      "        acc = tf.reduce_mean(tf.cast(correct_preds, tf.float32))    \n",
      "        res = tf.greater_equal(acc,tf.div(tf.cast(tf.size(acc), tf.float32),tf.constant(2.0)))\n",
      "        return res \n",
      "        \n",
      "    def conv2d(self,name, l_input, w, b,s=1):\n",
      "            return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(l_input, w, strides=[1, s, s, 1], padding='SAME'),b), name=name)\n",
      "         \n",
      "    def max_pool(self,name, l_input, k):\n",
      "            return tf.nn.max_pool(l_input, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME', name=name)\n",
      "         \n",
      "    def norm(self,name, l_input, lsize=4):\n",
      "            return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)\n",
      "        \n",
      "    def inference(self,_X, _weights, _biases, _dropout):\n",
      "        # Reshape input picture\n",
      "        _X = tf.reshape(_X, shape=[-1, 64, 64, 1])\n",
      "    \n",
      "        # Convolution Layer\n",
      "        conv1 = self.conv2d('conv1', _X, _weights['wc1'], _biases['bc1'])\n",
      "        # Max Pooling (down-sampling)\n",
      "        pool1 = self.max_pool('pool1', conv1, k=2)\n",
      "        # Apply Normalization\n",
      "        norm1 = self.norm('norm1', pool1, lsize=4)\n",
      "    \n",
      "        # Convolution Layer\n",
      "        conv2 = self.conv2d('conv2', norm1, _weights['wc2'], _biases['bc2'])\n",
      "        # Max Pooling (down-sampling)\n",
      "        pool2 = self.max_pool('pool2', conv2, k=2)\n",
      "        # Apply Normalization\n",
      "        norm2 = self.norm('norm2', pool2, lsize=4)\n",
      "        # Apply Dropout\n",
      "    \n",
      "        # Convolution Layer\n",
      "        conv3 = self.conv2d('conv3', norm2, _weights['wc3'], _biases['bc3'])\n",
      "       \n",
      "        conv4 = self.conv2d('conv4', conv3, _weights['wc4'], _biases['bc4'])\n",
      "\n",
      "        conv5 = self.conv2d('conv3', conv4, _weights['wc5'], _biases['bc5']) \n",
      "        pool5 = self.max_pool('pool5', conv5, k=2)\n",
      "\n",
      "        # Fully connected layer\n",
      "        dense1 = tf.reshape(pool5, [-1, _weights['wd1'].get_shape().as_list()[0]]) # Reshape pool5 output to fit dense layer input\n",
      "        dense1 = tf.nn.relu(tf.matmul(dense1, _weights['wd1']) + _biases['bd1'], name='fc1') # Relu activation\n",
      "        dense1 = tf.nn.dropout(dense1,_dropout)\n",
      "        \n",
      "        dense2 = tf.nn.relu(tf.matmul(dense1, _weights['wd2']) + _biases['bd2'], name='fc2') # Relu activation\n",
      "        dense2 = tf.nn.dropout(dense2,_dropout)\n",
      "        \n",
      "        dense3 = tf.nn.relu(tf.matmul(dense2, _weights['wd3']) + _biases['bd3'], name='fc3') # Relu activation\n",
      "        dense3 = tf.nn.dropout(dense3,_dropout)\n",
      "        \n",
      "        # Output, class logits\n",
      "        out = tf.matmul(dense3, _weights['out']) + _biases['out']\n",
      "        self.out = out\n",
      "        \n",
      "        return out,pool5,dense1,dense2,dense3\n",
      "    \n",
      "    # Store layers weight & bias\n",
      "    def initializevars(self):\n",
      "        self.weights = {\n",
      "            'wc1': tf.Variable(tf.random_normal([3, 3, 1, 128],stddev=0.01),trainable = self._istrainable),\n",
      "            'wc2': tf.Variable(tf.random_normal([3, 3, 128, 128],stddev=0.01),trainable = self._istrainable),\n",
      "            'wc3': tf.Variable(tf.random_normal([3, 3, 128, 128],stddev=0.01),trainable = self._istrainable),\n",
      "            'wc4': tf.Variable(tf.random_normal([3, 3, 128, 128],stddev=0.01),trainable = self._istrainable),\n",
      "            'wc5': tf.Variable(tf.random_normal([3, 3, 128, 128],stddev=0.01),trainable = self._istrainable),\n",
      "            'wd1': tf.Variable(tf.random_normal([np.int(np.ceil(64*64/(8*8))*128), 400],stddev=0.01),trainable = self._istrainable),\n",
      "            'wd2': tf.Variable(tf.random_normal([400, 400],stddev=0.01),trainable = self._istrainable),\n",
      "            'wd3': tf.Variable(tf.random_normal([400, 200],stddev=0.01),trainable = self._istrainable),\n",
      "            'out': tf.Variable(tf.random_normal([200, self._nclasses],stddev=0.01),trainable = self._istrainable)\n",
      "        }\n",
      "        self.biases = {\n",
      "            'bc1': tf.Variable(tf.zeros([128]),trainable = self._istrainable),\n",
      "            'bc2': tf.Variable(tf.zeros([128]),trainable = self._istrainable),\n",
      "            'bc3': tf.Variable(tf.zeros([128]),trainable = self._istrainable),\n",
      "            'bc4': tf.Variable(tf.zeros([128]),trainable = self._istrainable),\n",
      "            'bc5': tf.Variable(tf.zeros([128]),trainable = self._istrainable),\n",
      "            'bd1': tf.Variable(tf.zeros([400]),trainable = self._istrainable),\n",
      "            'bd2': tf.Variable(tf.zeros([400]),trainable = self._istrainable),\n",
      "            'bd3': tf.Variable(tf.zeros([200]),trainable = self._istrainable),\n",
      "            'out': tf.Variable(tf.zeros([self._nclasses]),trainable = self._istrainable)\n",
      "        }     \n",
      "        \n",
      "           "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "from tensorflow.models.rnn import seq2seq\n",
      "from tensorflow.models.rnn import rnn_cell\n",
      "from numpy import inf\n",
      "\n",
      "\n",
      "class MyRNN(TensorNet):\n",
      "        \n",
      "    def __init__(self,nclasses,rnnsize,batchsize,maxseq,baseCNN,inputdim,nlayers = 1):\n",
      "        \n",
      "        super().__init__(nclasses)  \n",
      "        \n",
      "        self.y = tf.placeholder(tf.int64, [batchsize, maxseq])\n",
      "\n",
      "        \n",
      "        self.cellsize = rnnsize\n",
      "        self._batchsize = batchsize\n",
      "        self._maxseq = maxseq\n",
      "        self._inputdim = inputdim\n",
      "        self._baseCNN = baseCNN\n",
      "        self.numlayers = nlayers\n",
      "        \n",
      "        self.early_stop = tf.placeholder(tf.int32, [self._batchsize])\n",
      "        \n",
      "        self.cost_w = tf.placeholder(tf.float32, [self._batchsize,self._maxseq]) \n",
      "        \n",
      "        initializer = tf.random_uniform_initializer(-.1,.1) \n",
      "\n",
      "        self.cell = tf.nn.rnn_cell.LSTMCell(self.cellsize, self._inputdim, initializer=initializer) \n",
      "        \n",
      "        self.cell = rnn_cell.DropoutWrapper(\n",
      "                                self.cell, output_keep_prob=self.keep_prob)\n",
      "        \n",
      "        if self.numlayers > 1:\n",
      "            allcells = [self.cell]\n",
      "            j = self.numlayers\n",
      "            while j > 1:\n",
      "                \n",
      "                cell2 = tf.nn.rnn_cell.LSTMCell(self.cellsize, self.cellsize, initializer=initializer) \n",
      "                cell2 = rnn_cell.DropoutWrapper(\n",
      "                                cell2, output_keep_prob=self.keep_prob)\n",
      "                allcells.append(cell2)\n",
      "                j = j - 1\n",
      "            self.cell = rnn_cell.MultiRNNCell(allcells)\n",
      "\n",
      "        self._initial_state = self.cell.zero_state(self._batchsize, tf.float32)\n",
      "\n",
      "\n",
      "    def inference(self):\n",
      "        \n",
      "        if self._inputdim == 64*64: \n",
      "            inputs = self.x\n",
      "        else:\n",
      "            pred,conv_end,d1,d2,d3 = self._baseCNN.inference(self.x, self._baseCNN.weights, self._baseCNN.biases, self.keep_prob)   \n",
      "            if (self._inputdim == 400):\n",
      "                inputs = d1\n",
      "            elif self._inputdim == 128*64:\n",
      "                inputs = conv_end\n",
      "\n",
      "        inputs = tf.nn.dropout(inputs, self.keep_prob)\n",
      "        \n",
      "        inputs = [tf.reshape(i, (self._batchsize, self._inputdim)) for i in tf.split(0, self._maxseq, inputs)]\n",
      "            \n",
      "        outputs, state = tf.nn.rnn(self.cell, inputs, initial_state=self._initial_state, \n",
      "                                   sequence_length=self.early_stop)\n",
      "        \n",
      "        return outputs\n",
      "    \n",
      "    def calc_cost(self):\n",
      "          \n",
      "        output = self.inference()  \n",
      "        output = tf.reshape(tf.concat(1, output), [-1, self.cellsize])\n",
      "        \"\"\"outputs: batchsize(outer index) * maxseq(inner index) X inputdim\"\"\"\n",
      "        \n",
      "        softmax_w = tf.get_variable(\"softmax_w\", [self.cellsize, self._nclasses])\n",
      "        softmax_b = tf.get_variable(\"softmax_b\", [self._nclasses])\n",
      "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
      "        \n",
      "        yy = tf.reshape(self.y, [-1])\n",
      "        loss = seq2seq.sequence_loss_by_example(logits=[logits],\n",
      "                                                targets=[yy],\n",
      "                                                weights=[tf.reshape(self.cost_w, [-1])],\n",
      "                                                average_across_timesteps=True)\n",
      "        \n",
      "        self.logits = logits\n",
      "        self.cost = tf.reduce_sum(loss) / self._batchsize   \n",
      "        \n",
      "        self.predictions = tf.argmax(logits,1)\n",
      "                \n",
      "        return self.cost,self.predictions \n",
      "            \n",
      "        \n",
      "        \"\"\"Make a single prediction per sequence of the batch\"\"\"\n",
      "        \"\"\"Input:\n",
      "            sess: Tensorflow session object\n",
      "            seq_xs: np.array, The batch of sequences. Dimensions: maxseq(outer) * batchsize(inner) X img_dim \n",
      "            seqlen: list, The length of each sequence in the batch. Dims: batchsize X 1 \n",
      "            seq_ys: np.array, Optional, Dim Batchsize X maxseqlen. \n",
      "           Output: \n",
      "            sequence_prediction: np.array, predicted label for each sequence in the batch\n",
      "            if seq_ys is provided:\n",
      "                corr_preds: np.array, 1 if a sequence is predicted correctly\n",
      "                cost: double, the perplexity\n",
      "            \"\"\"        \n",
      "    def predict(self,sess,seq_xs,dropout,seqlen,w,seq_ys = []):\n",
      "        \n",
      "        cost = -inf\n",
      "        if len(seq_ys) > 0:\n",
      "            preds,cost = sess.run( [self.predictions,self.cost], feed_dict={self.x: seq_xs, self.y: seq_ys, \n",
      "                            self.keep_prob: dropout, self.early_stop: seqlen, self.cost_w: w})      \n",
      "        else:\n",
      "            preds = sess.run( [self.predictions], feed_dict={self.x: seq_xs, #self.y: seq_ys, \n",
      "                            self.keep_prob: dropout, self.early_stop: seqlen, self.cost_w: w})      \n",
      "        \n",
      "        #Reshape to [batchsize X maxseq]\n",
      "        preds = np.reshape(preds,[self._batchsize,self._maxseq])\n",
      "        \n",
      "        print(preds)\n",
      "        #sum per sequence\n",
      "        sequence_prediction = np.sum(w*preds, 1)\n",
      "        \n",
      "        print('sequence output sum:')\n",
      "        print(sequence_prediction)\n",
      "        #find the sequences with equal votes (ambiguous)\n",
      "        iseq = sequence_prediction == np.array(seqlen)/2 \n",
      "        print('ambiguous elems:')\n",
      "        print(iseq)\n",
      "        #set the prediction label for not ambiguous sequences\n",
      "        sequence_prediction = sequence_prediction > np.array(seqlen)/2.          \n",
      "    \n",
      "        # for the ambiguous, label the sequence according to the label of the last element\n",
      "        if np.sum(iseq) > 0:\n",
      "            preds = preds[iseq,:]\n",
      "            print('ambiguous:')\n",
      "            print(preds)\n",
      "            seqlen = np.array(seqlen)\n",
      "            sequence_prediction[iseq] = preds[:,seqlen[iseq]-1];\n",
      "            \n",
      "            \n",
      "        #if seq_ys is provided, then output also correct predictions\n",
      "        corr_preds = []\n",
      "        if len(seq_ys) > 0:\n",
      "            corr_preds = seq_ys[:,0] == sequence_prediction\n",
      "\n",
      "        return sequence_prediction, corr_preds, cost \n",
      "            \n",
      "            \n",
      "    \"\"\"Creates weighting terms for prediction and loss\"\"\"\n",
      "    \"\"\"Should be called inside self.predict\"\"\"\n",
      "    \"\"\"input: length of each sequence in the batch to be processed \"\"\"\n",
      "    \"\"\"Output: w [batchsize X maxseq] \"\"\"        \n",
      "    def weighting(self,seqlen):\n",
      "        w=np.zeros([self._batchsize,self._maxseq])\n",
      "        for i in range(0,self._batchsize):\n",
      "            denom = np.sum(np.array([k for k in range(0,seqlen[i])]))\n",
      "            for j in range(0,self._maxseq):\n",
      "                w[i,j] = 1.*(j<seqlen[i])\n",
      "                #w[i,j] = ((j+1)/denom)*seqlen[i]*(j<seqlen[i])    \n",
      "        \n",
      "        return w\n",
      "        \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#########################\n",
      "######## MAIN ###########\n",
      "#########################"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 149
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "LOAD_CNN = True\n",
      "LOAD_RNN = False\n",
      "SAVE_MODEL = False\n",
      "EXPORT_MODEL = True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "LOAD_CNN_checkpoint_path = \"/home/lianos91/Desktop/training_patches/model9_2_125/\"\n",
      "n_classes = 2 # classes "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "checkfile = \"myRNNadamlr001_128.ckpt\"\n",
      "\n",
      "LOAD_CNN_checkpoint_path = \"/home/lianos91/Desktop/training_patches/model9_2_125/\"\n",
      "save_checkpoint_path = \"/home/lianos91/Desktop/training_patches/model12batched/\"\n",
      "\n",
      "# Framework Parameters\n",
      "learning_rate = 0.01\n",
      "training_steps = 120000\n",
      "display_step = 100\n",
      "batchsize = 2\n",
      "maxseq = 12\n",
      "\n",
      "# Create readers\n",
      "trainreader = MyRNNreader(\"train_tinyTiny/\",istest=False,maxsequence=maxseq)\n",
      "testreader = MyRNNreader(\"train_tinyTiny/\",istest=True,maxsequence=maxseq)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Network Parameters\n",
      "dropout = .6#0.70 # Dropout, probability to keep units\n",
      "nlayers = 2\n",
      "rnnsize = 256\n",
      "#batchsize = 2\n",
      "#maxseq = 12\n",
      "trainreader._index = 0;\n",
      "testreader._index = 0;"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'trainreader' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-7-05b2d218b4d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#batchsize = 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#maxseq = 12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrainreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtestreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'trainreader' is not defined"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "LOAD_CNN_checkpoint_path = \"/home/lianos91/Desktop/training_patches/model9_2_125/\"\n",
      "n_classes = 2 # classes \n",
      "\n",
      "CNNbase = MyCNN(n_classes,istrainable=False)\n",
      "\n",
      "max_grad_norm = 5\n",
      "        \n",
      "#_lr = tf.Variable(0.0, trainable=False)\n",
      "\n",
      "cost,_,_ = CNNbase.calc_cost()\n",
      "\n",
      "tvars = tf.trainable_variables()\n",
      "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars,aggregation_method=2),\n",
      "                                      max_grad_norm)\n",
      "\n",
      "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-08, \n",
      "                                   use_locking=False, name='Adam')\n",
      "train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
      "\n",
      "print(\"[train_script]: Optimization is set\")\n",
      "\n",
      "\n",
      "sess = tf.InteractiveSession()\n",
      "saverCNN = tf.train.Saver(tf.all_variables())\n",
      "\n",
      "init1 = tf.initialize_all_variables()\n",
      "sess.run(init1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Exception ignored in: <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7f4491e3ae80>>\n",
        "Traceback (most recent call last):\n",
        "  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 138, in __del__\n",
        "    self.close()\n",
        "  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 784, in close\n",
        "    self._default_session.__exit__(None, None, None)\n",
        "  File \"/usr/lib/python3.4/contextlib.py\", line 66, in __exit__\n",
        "    next(self.gen)\n",
        "  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\", line 2978, in get_controller\n",
        "    assert self.stack[-1] is default\n",
        "AssertionError: \n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"Construct and load pretrained CNN model\"\"\"\n",
      "\n",
      "\n",
      "if LOAD_CNN:\n",
      "    ckpt = tf.train.get_checkpoint_state(LOAD_CNN_checkpoint_path)\n",
      "    if ckpt and ckpt.model_checkpoint_path:\n",
      "        print(\"[train_script]: LOADED CNN!\")\n",
      "        saverCNN.restore(sess, ckpt.model_checkpoint_path)\n",
      "    else:\n",
      "        print(\"[train_script]: Failed to LOAD CNN!\")\n",
      "        raise SystemExit  \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[train_script]: LOADED CNN!\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tensor(\"Placeholder:0\", shape=(?, 4096), dtype=float32)\n",
        "Tensor(\"Placeholder_2:0\", dtype=float32)\n",
        "Tensor(\"Placeholder_3:0\", shape=(?, 2), dtype=float32)\n",
        "Tensor(\"add_3:0\", shape=(?, 2), dtype=float32)\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"Construct RNN\"\"\"\n",
      "rnn_net = MyRNN(n_classes,rnnsize,batchsize,maxseq,CNNbase,64*64,nlayers)\n",
      "# Define loss \n",
      "cost,prds = rnn_net.calc_cost()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"Define Optimization settings\"\"\"\n",
      "#decay=0.0005, momentum=0.9\n",
      "#batch = tf.Variable(0,trainable=False)\n",
      "#learning_rate = tf.train.exponential_decay(  0.00051,\n",
      "#                                             #Base learning\n",
      "#                                              #rate.\n",
      "#                                              batch * 128,  \n",
      "#                                             # Current index into the dataset.\n",
      "#                                             trainreader.dir_size,\n",
      "#                                                        #Decay step.\n",
      "#                                                        0.95,              \n",
      "#                                                         # Decay rate.\n",
      "#                                                         staircase=True)\n",
      "# Use simple momentum for the optimization.\n",
      "#optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
      "#                                     0.019).minimize(cost,\n",
      "#                                                   global_step=batch)\n",
      "\n",
      "max_grad_norm = 5\n",
      "        \n",
      "#_lr = tf.Variable(0.0, trainable=False)\n",
      "tvars = tf.trainable_variables()\n",
      "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars,aggregation_method=2),\n",
      "                                      max_grad_norm)\n",
      "        \n",
      "#optimizer = tf.train.GradientDescentOptimizer(_lr)\n",
      "\n",
      "##get variables e\n",
      "temp = set(tf.all_variables())\n",
      "\n",
      "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-08, \n",
      "                                   use_locking=False, name='Adam')\n",
      "train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
      "\n",
      "print(\"[train_script]: Optimization is set\")\n",
      "\n",
      "\n",
      "#remaining variables = ADAM variables + trainable variables\n",
      "remaining_vars = (set(tf.all_variables()) - temp ) | set(tf.trainable_variables())\n",
      "init2 = tf.initialize_variables(remaining_vars)\n",
      "\n",
      "# Build the summary operation based on the TF collection of Summaries.\n",
      "#summary_op = tf.merge_all_summaries()\n",
      "#summaries\n",
      "#cost_summ = tf.scalar_summary(\"cost\", cost)\n",
      "\n",
      "saverRNN = tf.train.Saver(tf.all_variables())\n",
      "\n",
      "#merged = tf.merge_all_summaries()\n",
      "#summ_writer = tf.train.SummaryWriter(trainreader.dirpath)\n",
      "sess.run(init2)\n",
      "\n",
      "print(\"[train_script]: Initialized\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[train_script]: Optimization is set\n",
        "[train_script]: Initialized"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "step = 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"Load RNN\"\"\"\n",
      "\n",
      "   \n",
      "if LOAD_RNN:\n",
      "    ckpt = tf.train.get_checkpoint_state(save_checkpoint_path)\n",
      "    print(ckpt.model_checkpoint_path)\n",
      "    if ckpt and ckpt.model_checkpoint_path:\n",
      "        print(\"[train_script]: LOADED RNN\")\n",
      "        saverRNN.restore(sess, ckpt.model_checkpoint_path)\n",
      "    else:\n",
      "        print(\"[train_script]: failed to load\")\n",
      "        raise SystemExit\n",
      "    last_checkpoint_path = ckpt.model_checkpoint_path\n",
      "    step = 1+int(last_checkpoint_path[last_checkpoint_path.rindex('-')+1:])\n",
      "    print(step)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 161
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if EXPORT_MODEL:\n",
      "    from tensorflow_serving.session_bundle import exporter\n",
      "    # Export model\n",
      "    export_path = '/home/lianos91/workspace/SemesterProject/'\n",
      "    #saver = tf.train.Saver(sharded=True)\n",
      "    model_exporter = exporter.Exporter(saverCNN) # or saver.CNN to save the CNN model\n",
      "    signature = exporter.classification_signature(input_tensor=rnn_net.x, scores_tensor=rnn_net.predictions)\n",
      "    #outputs the prediction for each frame of the sequences. Need postprocessing\n",
      "    model_exporter.init(sess.graph.as_graph_def(),\n",
      "              default_graph_signature=signature)\n",
      "    model_exporter.export(export_path, tf.constant(FLAGS.export_version), sess)\n",
      "    #print 'Done exporting!'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named 'tensorflow_serving'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-12-5fe1f97b2697>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mEXPORT_MODEL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_serving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_bundle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexporter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Export model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mexport_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/lianos91/workspace/SemesterProject/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#saver = tf.train.Saver(sharded=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mImportError\u001b[0m: No module named 'tensorflow_serving'"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"[train_script]: Start training\")\n",
      "   \n",
      "# Keep training until reach max iterations\n",
      "tmp_train_acc = []\n",
      "tmp_test_acc = []\n",
      "train_cost = []\n",
      "trainreader.invoke_thread()\n",
      "e = 0\n",
      "#while step < training_steps:"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[train_script]: Start training\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "seqlen,seq_xs, seq_ys = trainreader.read_batch(batchsize)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.770883910501888\n",
        "00102b0933db41a2b6856899b94acb12_7_0_46_0.png\n",
        "reading sequence..\n",
        "00102b0933db41a2b6856899b94acb12_7_0_46_0.png\n",
        "00102b0933db41a2b6856899b94acb12_7_1_46_0.png\n",
        "00102b0933db41a2b6856899b94acb12_7_2_46_0.png\n",
        "00102b0933db41a2b6856899b94acb12_7_3_46_0.png\n",
        "00102b0933db41a2b6856899b94acb12_7_4_46_0.png\n",
        "00102b0933db41a2b6856899b94acb12_7_5_46_0.png\n",
        "00102b0933db41a2b6856899b94acb12_7_6_46_0.png\n",
        "00102b0933db41a2b6856899b94acb12_7_7_46_0.png"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "00102b0933db41a2b6856899b94acb12_7_8_46_0.png\n",
        "00102b0933db41a2b6856899b94acb12_7_9_46_0.png\n",
        "00104216ec814d5cb6fce05fa123c96e_0_0_4_0.png\n",
        "seq. length: 10\n",
        "0.5694944127453757\n",
        "00104216ec814d5cb6fce05fa123c96e_0_0_4_0.png\n",
        "reading sequence..\n",
        "00104216ec814d5cb6fce05fa123c96e_0_0_4_0.png\n",
        "00104216ec814d5cb6fce05fa123c96e_0_1_4_0.png\n",
        "00104216ec814d5cb6fce05fa123c96e_0_2_4_0.png\n",
        "00104216ec814d5cb6fce05fa123c96e_0_3_4_0.png\n",
        "00104216ec814d5cb6fce05fa123c96e_1_0_4_0.png\n",
        "seq. length: 4\n"
       ]
      }
     ],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(seqlen)\n",
      "print(maxseq)\n",
      "print(batchsize)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[10, 4]\n",
        "12\n",
        "2\n"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w = rnn_net.weighting(seqlen)\n",
      "print(w)\n",
      "print(seq_ys)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.]\n",
        " [ 1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
        "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1. -1.]\n",
        " [ 0.  0.  0.  0. -1. -1. -1. -1. -1. -1. -1. -1.]]\n"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "       \n",
      "\"\"\"seq_xs: maxseq(outer) * batchsize(inner) X img_dim \"\"\"\n",
      "\"\"\"seq_ys: batchsize X maxseq \"\"\"\n",
      "\"\"\"w     : batchsize X maxseq \"\"\"\n",
      "\"\"\"seqlen: batchsize X 1      \"\"\"\n",
      "\n",
      "# Fit training using sequence data\n",
      "sess.run( [train_op], feed_dict={rnn_net.x: seq_xs, rnn_net.y: seq_ys, \n",
      "                               rnn_net.keep_prob: dropout, rnn_net.early_stop: seqlen, \n",
      "                               rnn_net.cost_w: w})  \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 101,
       "text": [
        "[None]"
       ]
      }
     ],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\"\"\"\n",
      "\"\"\"get training and test error\"\"\"    \n",
      "logits = sess.run( [rnn_net.logits], feed_dict={rnn_net.x: seq_xs, rnn_net.y: seq_ys, \n",
      "                               rnn_net.keep_prob: 1., rnn_net.early_stop: seqlen, \n",
      "                               rnn_net.cost_w: w})    \n",
      "pr,corr_predictions,cst = rnn_net.predict(sess,seq_xs,1.,seqlen,w,seq_ys)\n",
      "print(cst)\n",
      "print(pr)\n",
      "print(corr_predictions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0 0 0 0 0 0 0 0 0 0 1 1]\n",
        " [0 0 0 0 1 1 1 1 1 1 1 1]]\n",
        "sequence output sum:\n",
        "[ 0.  0.]\n",
        "ambiguous elems:\n",
        "[False False]\n",
        "0.874937\n",
        "[False False]\n",
        "[ True  True]\n"
       ]
      }
     ],
     "prompt_number": 102
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "logits"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ True False]\n",
        "[ True  True]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 59,
       "text": [
        "[array([[-0.26771477, -0.30831426],\n",
        "       [-0.43853402, -0.22959548],\n",
        "       [-0.74760956,  0.00346002],\n",
        "       [-0.86644602,  0.08246173],\n",
        "       [-0.99034023,  0.16701475],\n",
        "       [-1.24516487,  0.40141249],\n",
        "       [-1.42180967,  0.55386037],\n",
        "       [-1.55409598,  0.6330021 ],\n",
        "       [-1.62903023,  0.73747301],\n",
        "       [-1.66915584,  0.78017086],\n",
        "       [-1.67631936,  0.80178398],\n",
        "       [-1.6722157 ,  0.84002787],\n",
        "       [ 0.86759239, -1.30543125],\n",
        "       [ 0.92163616, -1.40453959],\n",
        "       [ 1.59902024, -1.93700826],\n",
        "       [ 1.39764786, -1.8246237 ],\n",
        "       [ 1.45967269, -1.94057071],\n",
        "       [-0.51774722,  0.03682286],\n",
        "       [-0.51774722,  0.03682286],\n",
        "       [-0.51774722,  0.03682286],\n",
        "       [-0.51774722,  0.03682286],\n",
        "       [-0.51774722,  0.03682286],\n",
        "       [-0.51774722,  0.03682286],\n",
        "       [-0.51774722,  0.03682286]], dtype=float32)]"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use test set\n",
      "\n",
      "seqlen_test,seq_xs_test, seq_ys_test = testreader.read_batch(batchsize)\n",
      "\n",
      "w=[]\n",
      "for j in range(0,maxseq):\n",
      "    wtemp = np.zeros([batchsize],dtype=np.float32)\n",
      "    for i in range(0,batchsize):\n",
      "        wtemp[i] = 1.*(j<seqlen_test[i])\n",
      "    w.extend(wtemp)   \n",
      "corr_prs = sess.run( [are_corr_preds], feed_dict={rnn_net.x: seq_xs_test, rnn_net.y: seq_ys_test, \n",
      "                    rnn_net.keep_prob: 1., rnn_net.early_stop: seqlen_test, rnn_net.cost_w: w})          \n",
      "\n",
      "#Reshape to [maxseq X batchsize]\n",
      "corr_prs = np.reshape(corr_prs,[maxseq,batchsize])\n",
      "w = np.reshape(np.array(w),[maxseq,batchsize])        \n",
      "pr = np.sum(w*corr_prs, 0)>= np.array(seqlen_test)/2.          \n",
      "tmp_test_acc.extend(pr)\n",
      "\n",
      "if np.sum(np.isnan(pr)):\n",
      "    print(\"____Diverged____\")\n",
      "    raise SystemExit   \n",
      "# display and write summary\n",
      "if step % display_step == 0:\n",
      "#if trainreader._epochs > e: (for train_small)\n",
      "#   e = e+1\n",
      "    #output mean training accuracy of past sequences\n",
      "    m = np.mean(np.array(tmp_train_acc,np.float32))\n",
      "    m2 = np.mean(np.array(tmp_test_acc,np.float32))\n",
      "    m3 = np.mean(np.array(train_cost,np.float32))\n",
      "    m4 = np.std(np.array(train_cost,np.float32))\n",
      "    print(\"[train_script]: Step: \"+str(step)+ \", train_acc {:.3f}\".format(m)+ \", test_acc {:.3f}\".format(m2))\n",
      "    print(\"train_mean_cost {:.3f}\".format(m3)+ \", train_cost_std {:.3f}\".format(m4) )\n",
      "    tmp_train_acc = []\n",
      "    tmp_test_acc = []\n",
      "    train_cost = []\n",
      "    testreader.index = 0\n",
      "    #write summary\n",
      "    #summ_writer.add_summary(m, step)\n",
      "       \n",
      "#checkpoint save\n",
      "if step % (5*display_step) == 0 and SAVE_MODEL:\n",
      "    print(\"[train_script]: checkpoint\")\n",
      "    checkpoint_path = os.path.join(save_checkpoint_path, checkfile)\n",
      "    saverRNN.save(sess, checkpoint_path, global_step=step)        \n",
      "step += 1\n",
      "    \n",
      "trainreader.terminate = True\n",
      "print(\"[train_script]: Optimization Finished!\")\n",
      "if SAVE_MODEL:\n",
      "    print(\"[train_script]: checkpoint\")\n",
      "    checkpoint_path = os.path.join(save_checkpoint_path, checkfile)\n",
      "    saverRNN.save(sess, checkpoint_path, global_step=step)\n",
      "\n",
      "#acquire test set\n",
      "\n",
      "print(\"[train_script]: Testing\")\n",
      "\n",
      "step=0\n",
      "summ = 0\n",
      "\n",
      "testreader.index = 0\n",
      "\n",
      "while testreader.epochs_compl < 1:\n",
      "    #seqlen,seq_xs, seq_ys = testreader.read_sequence()\n",
      "    seqlen,seq_xs, seq_ys = testreader.read_batch(batchsize)\n",
      "    w=[]\n",
      "    for j in range(0,maxseq):\n",
      "        wtemp = np.zeros([batchsize],dtype=np.float32)\n",
      "        for i in range(0,batchsize):\n",
      "            wtemp[i] = 1.*(j<seqlen[i])\n",
      "        w.extend(wtemp)\n",
      "    \n",
      "    \"\"\"w: list of batch-sized 1D tensors (list size = timesteps)\"\"\"\n",
      "    corr_prs = sess.run( [are_corr_preds], feed_dict={rnn_net.x: seq_xs, rnn_net.y: seq_ys, \n",
      "                                              rnn_net.keep_prob: 1.,rnn_net.early_stop:seqlen,\n",
      "                                              rnn_net.cost_w: w})\n",
      "    \n",
      "    pr = np.sum(np.array(corr_prs*w)) >= np.array(seqlen)/2.\n",
      "    tmp_train_acc.append(np.float(pr))\n",
      "    if step % (3*display_step) == 0:\n",
      "        print(\"partial mean acc: \" + \"{:.3f}\".format(np.mean(np.array(tmp_train_acc))))\n",
      "        tmp_train_acc = []\n",
      "    summ += pr\n",
      "    step += 1\n",
      "\n",
      "print(\"Testing Accuracy:\", \"{:.5f}\".format(summ/step))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}